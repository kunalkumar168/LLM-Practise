{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wO50kUe8u84n",
        "outputId": "6c262ff1-0a03-4015-c58b-eb5613c375e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "9zszJK4HvOUp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device = {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlqgHA-BvOy9",
        "outputId": "93bd5a0c-5c86-4794-f1e4-2b2eb58073eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UHQ0bQh0tPQY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, attn_dim, num_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert attn_dim %  num_head == 0, \"attn_dim should be divisible by num_head\"\n",
        "        self.attn_dim = attn_dim\n",
        "        self.num_head = num_head\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3*attn_dim)\n",
        "        self.out_proj = nn.Linear(attn_dim, embed_dim)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        bs, seq_len, embed_dim = X.shape\n",
        "        head_dim = self.attn_dim//self.num_head\n",
        "        QKV = self.qkv_proj(X).view(bs, seq_len, self.num_head, 3*head_dim).transpose(1,2)\n",
        "        Q, K, V = torch.chunk(QKV, chunks=3, dim=-1) # bs, #head, seq_len, head_dim\n",
        "        attn_score = torch.matmul(Q, K.transpose(-2,-1))/(head_dim**0.5) # bs, #head, seq_len, seq_len\n",
        "        if mask is not None:\n",
        "            masking = torch.tril(torch.ones(seq_len, seq_len).unsqueeze(0).unsqueeze(0)).to(X.device) # 1, 1, seq_len, seq_len\n",
        "            attn_score = attn_score.masked_fill(masking==0, float(\"-inf\")) # bs, #head, seq_len, seq_len\n",
        "        attn_score = F.softmax(attn_score, dim=-1)\n",
        "        output = torch.matmul(attn_score, V) # bs, #head, seq_len, head_dim\n",
        "        concated_output = output.transpose(1,2).contiguous().view(bs, seq_len, self.attn_dim)\n",
        "        return self.out_proj(concated_output)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=512):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        bs, seq_len, embed_dim = X.shape\n",
        "        positions = torch.arange(0, seq_len, device=X.device).unsqueeze(0) # 1, seq_len\n",
        "        pos_embed = self.pos_embed(positions) # 1, seq_len, embed_dim\n",
        "        return X + pos_embed\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(FeedForwardLayer, self).__init__()\n",
        "        self.up_projection = nn.Linear(embed_dim, 3*embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down_projection = nn.Linear(3*embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.down_projection(self.relu(self.up_projection(X)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, attn_dim, num_head):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_norm = nn.LayerNorm(embed_dim)\n",
        "        self.multi_head_attn = MultiHeadAttention(embed_dim, attn_dim, num_head)\n",
        "        self.ffn = FeedForwardLayer(embed_dim)\n",
        "        self.ffn_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        residual = X\n",
        "        X = self.attn_norm(X)\n",
        "        X = residual + self.multi_head_attn(X, mask)\n",
        "\n",
        "        residual = X\n",
        "        X = self.ffn_norm(X)\n",
        "        X = residual + self.ffn(X)\n",
        "        return X\n",
        "\n",
        "class Decoder(nn.Module): # masked-multi-head and ffn_layer\n",
        "    def __init__(self, vocab_size, embed_dim, attn_dim, num_head, num_blocks):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, embed_dim) # vocab_size, embed_dim\n",
        "        self.positional_embedding = PositionalEmbedding(embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(embed_dim, attn_dim, num_head) for _ in range(num_blocks)])\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, mask=None):\n",
        "        X = self.token_embed(input_ids) # bs, seq_len, embed_dim\n",
        "        X = self.positional_embedding(X)\n",
        "        for blocks in self.transformer_blocks:\n",
        "            X = blocks(X, mask=mask) # bs, seq_len, embed_dim\n",
        "        logits = self.lm_head(X) # bs, seq_len, vocab_size\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PrepareDataset(Dataset):\n",
        "    def __init__(self, input_ids, seq_len):\n",
        "        super(PrepareDataset, self).__init__()\n",
        "        self.samples = []\n",
        "        for ids in input_ids:\n",
        "            if len(ids)<2:\n",
        "                continue\n",
        "            for i in range(1, len(ids)):\n",
        "                input_seq = ids[:i]\n",
        "                target_seq = ids[1:i+1]\n",
        "                # if PAD is needed\n",
        "                if len(input_seq) < seq_len:\n",
        "                    pad_len = seq_len - len(input_seq)\n",
        "                    input_seq = torch.cat([input_seq, torch.zeros(pad_len, dtype=torch.long)], dim=-1)\n",
        "                    target_seq = torch.cat([target_seq, torch.zeros(pad_len, dtype=torch.long)], dim=-1)\n",
        "                else:\n",
        "                    input_seq = input_seq[-seq_len:] # trim extra sequence\n",
        "                    target_seq = target_seq[-seq_len:] # trim extra sequence\n",
        "                self.samples.append((input_seq, target_seq))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ],
      "metadata": {
        "id": "9WDrkLOKtbW0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences = [\n",
        "#     \"hello world\",\n",
        "#     \"hello there\",\n",
        "#     \"how are you\",\n",
        "#     \"good morning\",\n",
        "#     \"good night\",\n",
        "#     \"see you soon\",\n",
        "#     \"have a nice day\",\n",
        "#     \"what are you doing\",\n",
        "#     \"where are you going\",\n",
        "#     \"thank you very much\",\n",
        "#     \"with great power comes great responsibility\",\n",
        "# ]\n",
        "\n",
        "# class PrepareVocabulary:\n",
        "#     def __init__(self, sample_sentence):\n",
        "#         self.sentence = sample_sentence\n",
        "\n",
        "#     def get_ids(self):\n",
        "#         all_words = \" \".join(self.sentence)\n",
        "#         words_list = all_words.split(' ')\n",
        "#         unique_words = list(set(words_list))\n",
        "#         unique_words.sort()\n",
        "#         token_to_id = {unique_words[i]:i for i in range(len(unique_words))}\n",
        "#         id_to_token = {i:unique_words[i] for i in range(len(unique_words))}\n",
        "#         vocab_size = len(unique_words)\n",
        "#         return vocab_size, token_to_id, id_to_token"
      ],
      "metadata": {
        "id": "mlMC7nW-tiIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_decoder(model, train_data):\n",
        "    print(\"#\"*10, \" Model Training \", \"#\"*10)\n",
        "    model.train()\n",
        "    epochs = 5\n",
        "    lr = 1e-4\n",
        "    optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1,epochs+1):\n",
        "        batch_loss = 0\n",
        "        for input_seq, target_seq in train_data:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "            pred_seq = model(input_seq, mask=True) # bs, seq_len, vocab_size\n",
        "            pred_seq = pred_seq.reshape(-1, pred_seq.size(-1)) # bs * seq_len, vocab_size\n",
        "            target_seq = target_seq.view(-1) # bs * seq_len\n",
        "            loss = criterion(pred_seq, target_seq)\n",
        "            batch_loss += loss.item()\n",
        "            optimiser.zero_grad()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "        print(f\"For epoch : {epoch}/{epochs}, training error: {batch_loss/len(train_data)}\")\n",
        "    print(\"#\"*30)"
      ],
      "metadata": {
        "id": "k4oa8X4WtpXw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decoder(model, input_ids, id_to_token=None, tokenizer=None):\n",
        "    print(\"#\"*10, \" Model Evaluation \", \"#\"*10)\n",
        "    model.eval()\n",
        "    logits = model(input_ids, mask=True) # bs, seq_len, vocab_size\n",
        "    next_token_logits = logits[:, -1, :] # 1, vocab_size\n",
        "    next_token_prob = F.softmax(next_token_logits, dim=-1)\n",
        "    # predicted_id = torch.argmax(next_token_prob, dim=-1).item()\n",
        "    topk_probs, topk_ids = torch.topk(next_token_prob, k=3)\n",
        "    print(f\"Input tokens : {input}\")\n",
        "    for i in range(3):\n",
        "        if id_to_token is not None:\n",
        "            print(f\"Next Prediction : {id_to_token[topk_ids[0,i].item()]} and its Prob: {topk_probs[0,i].item()}\")\n",
        "        else:\n",
        "            print(f\"Next Prediction : {tokenizer.decode([topk_ids[0,i].item()])} and its Prob: {topk_probs[0,i].item()}\")\n",
        "    print(\"#\"*30)"
      ],
      "metadata": {
        "id": "_pCURbVots3V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary = PrepareVocabulary(sample_sentences)\n",
        "# vocab_size, token_to_id, id_to_token = vocabulary.get_ids()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# text = \" \".join(input)\n",
        "# encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "# input_ids = encoded_input.input_ids\n",
        "# print(\"input ids :\", input_ids)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "dataset = load_dataset(\"ag_news\", split=\"train[:50]\")  # Try a smaller subset first\n",
        "sentences = [x['text'] for x in dataset]\n",
        "print(sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li8-r_2WtwMc",
        "outputId": "838f2f11-4f0d-4be2-cc90-c54f0385ef49"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 512\n",
        "attn_dim = 512\n",
        "num_head = 16\n",
        "decoder_layers = 10\n",
        "\n",
        "model = Decoder(vocab_size, embed_dim, attn_dim, num_head, decoder_layers).to(device)"
      ],
      "metadata": {
        "id": "OxYGZz5duHR0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdgq87zYwv1J",
        "outputId": "c6b563f4-0389-461d-f325-269e14e073df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder(\n",
            "  (token_embed): Embedding(50257, 512)\n",
            "  (positional_embedding): PositionalEmbedding(\n",
            "    (pos_embed): Embedding(512, 512)\n",
            "  )\n",
            "  (transformer_blocks): ModuleList(\n",
            "    (0-9): 10 x TransformerBlock(\n",
            "      (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (multi_head_attn): MultiHeadAttention(\n",
            "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn): FeedForwardLayer(\n",
            "        (up_projection): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (relu): ReLU()\n",
            "        (down_projection): Linear(in_features=1536, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=50257, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "for sentence in sentences:\n",
        "    # splitted = sentence.split()\n",
        "    # input_ids.append(torch.tensor([token_to_id[token] for token in splitted], dtype=torch.long)) # len(sample_sentences), seq_len\n",
        "    input_id = tokenizer(sentence, return_tensors=\"pt\").input_ids.squeeze(0)\n",
        "    input_ids.append(input_id)\n",
        "batch_size = 4\n",
        "seq_len = 32\n",
        "train_dataset = PrepareDataset(input_ids, seq_len)\n",
        "train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(\"Train data size: \",len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7cA5S2QuBUn",
        "outputId": "0abc4244-632b-47c6-c494-88d81d6ff2e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size:  572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_decoder(model, train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLqxiX-Vw4eY",
        "outputId": "053495dd-5363-43c4-ce29-d138223a7455"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########  Model Training  ##########\n",
            "For epoch : 1/5, training error: 0.2628570656233933\n",
            "For epoch : 2/5, training error: 0.07091369062290287\n",
            "For epoch : 3/5, training error: 0.05843537144048585\n",
            "For epoch : 4/5, training error: 0.051323041747653064\n",
            "For epoch : 5/5, training error: 0.0511371527156714\n",
            "##############################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"Russian Alien\"\n",
        "# input_ids = torch.tensor([token_to_id[word] for word in input.split()], dtype=torch.long).unsqueeze(0) # 1, seq_len\n",
        "input_ids = tokenizer(input, return_tensors=\"pt\").input_ids\n",
        "test_decoder(model, input_ids.to(device), None, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P06nVGAZt8IR",
        "outputId": "2486cd30-f244-4ab8-8d93-4a734e7a0139"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########  Model Evaluation  ##########\n",
            "Input tokens : Russian Alien\n",
            "Next Prediction : , and its Prob: 0.6040710806846619\n",
            "Next Prediction :  ( and its Prob: 0.14017179608345032\n",
            "Next Prediction :  everything and its Prob: 0.05243949964642525\n",
            "##############################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"What's in a Name?\"\n",
        "input_ids = tokenizer(input, return_tensors=\"pt\").input_ids\n",
        "test_decoder(model, input_ids.to(device), None, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47_dNvsqt-Mz",
        "outputId": "61a1c6f8-8ec1-477e-f23f-0f1114dc5b58"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########  Model Evaluation  ##########\n",
            "Input tokens : What's in a Name?\n",
            "Next Prediction :  More and its Prob: 0.21408550441265106\n",
            "Next Prediction :  \\ and its Prob: 0.18890926241874695\n",
            "Next Prediction :  The and its Prob: 0.1826314926147461\n",
            "##############################\n"
          ]
        }
      ]
    }
  ]
}